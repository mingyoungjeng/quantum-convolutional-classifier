{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "rng = np.random.default_rng()\n",
    "\n",
    "%run src/qcnn.py\n",
    "%run src/qcnn_old.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# qcnn = QCNN((32, 32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# qcnn.predict(rng.random(2**8), rng.random(420))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import MNIST, FashionMNIST\n",
    "from torch.optim import SGD\n",
    "from torch.nn import CrossEntropyLoss, MSELoss\n",
    "from pennylane import NesterovMomentumOptimizer\n",
    "\n",
    "# accuracy = qcnn.run(FashionMNIST, SGD, CrossEntropyLoss())\n",
    "\n",
    "# print(f\"{accuracy:.3%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New ansatz: accuracy=93.550%\n"
     ]
    }
   ],
   "source": [
    "qcnn = QCNN((32, 32))\n",
    "\n",
    "accuracy = qcnn.run(FashionMNIST, SGD, CrossEntropyLoss())\n",
    "print(f\"New ansatz: {accuracy=:.3%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m qcnn \u001b[39m=\u001b[39m QCNN_Old((\u001b[39m32\u001b[39m, \u001b[39m32\u001b[39m))\n\u001b[0;32m----> 3\u001b[0m accuracy \u001b[39m=\u001b[39m qcnn\u001b[39m.\u001b[39;49mrun(FashionMNIST, SGD, CrossEntropyLoss())\n\u001b[1;32m      4\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mOld ansatz: \u001b[39m\u001b[39m{\u001b[39;00maccuracy\u001b[39m=:\u001b[39;00m\u001b[39m.3%\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Downloads/thesis/src/qcnn_old.py:45\u001b[0m, in \u001b[0;36mQCNN_Old.run\u001b[0;34m(self, dataset, optimizer, cost_fn)\u001b[0m\n\u001b[1;32m     43\u001b[0m num_layers \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(np\u001b[39m.\u001b[39mceil(np\u001b[39m.\u001b[39mlog2(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_qubits)))\n\u001b[1;32m     44\u001b[0m new_params \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mrandn(\u001b[39m17\u001b[39m \u001b[39m*\u001b[39m num_layers, requires_grad\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m---> 45\u001b[0m parameters \u001b[39m=\u001b[39m train(\n\u001b[1;32m     46\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mqnode,\n\u001b[1;32m     47\u001b[0m     optimizer,\n\u001b[1;32m     48\u001b[0m     training_dataloader,\n\u001b[1;32m     49\u001b[0m     cost_fn,\n\u001b[1;32m     50\u001b[0m     initial_parameters\u001b[39m=\u001b[39;49mnew_params,\n\u001b[1;32m     51\u001b[0m )\n\u001b[1;32m     53\u001b[0m accuracy \u001b[39m=\u001b[39m test(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mqnode, parameters, testing_dataloader)\n\u001b[1;32m     55\u001b[0m \u001b[39mreturn\u001b[39;00m accuracy\n",
      "File \u001b[0;32m~/Downloads/thesis/src/training.py:35\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(fn, optimizer, training_dataloader, cost_fn, initial_parameters, total_params)\u001b[0m\n\u001b[1;32m     32\u001b[0m     predictions \u001b[39m=\u001b[39m predictions\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m)\n\u001b[1;32m     34\u001b[0m cost \u001b[39m=\u001b[39m cost_fn(predictions, labels)\n\u001b[0;32m---> 35\u001b[0m cost\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     36\u001b[0m opt\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     38\u001b[0m \u001b[39m# if (i == 0) or ((i + 1) % 100 == 0) or (i + 1 == len(training_dataloader)):\u001b[39;00m\n\u001b[1;32m     39\u001b[0m \u001b[39m#     print(f\"{i+1}/{len(training_dataloader)}: {cost=:.03f}\")\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_tensor.py:488\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    478\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    479\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    480\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    481\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    486\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    487\u001b[0m     )\n\u001b[0;32m--> 488\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    489\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    490\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/autograd/__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    192\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    194\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 197\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    198\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    199\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "qcnn = QCNN_Old((32, 32))\n",
    "\n",
    "accuracy = qcnn.run(FashionMNIST, SGD, CrossEntropyLoss())\n",
    "print(f\"Old ansatz: {accuracy=:.3%}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c7967d85baf232544f00baec60c53642b018367ac4c489b1a4dde60b922cc6fa"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
