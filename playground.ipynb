{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "from torchvision.datasets import MNIST, FashionMNIST, CIFAR10\n",
                "from torch.optim import SGD\n",
                "from torch.nn import CrossEntropyLoss, MSELoss\n",
                "from pennylane import NesterovMomentumOptimizer\n",
                "# rng = np.random.default_rng()\n",
                "\n",
                "%run src/qcnn.py\n",
                "%run src/qcnn_old.py\n",
                "\n",
                "dims = (16, 16), (28, 28), (32, 32)\n",
                "num_trials = 10"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Using cpu device\n",
                        "len(params)=12\n"
                    ]
                },
                {
                    "ename": "KeyboardInterrupt",
                    "evalue": "",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
                        "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
                        "Cell \u001b[0;32mIn[2], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m qcnn \u001b[39m=\u001b[39m QCNN(dim, ansatz\u001b[39m=\u001b[39mAnsatz1())\n\u001b[1;32m      6\u001b[0m \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_trials):\n\u001b[0;32m----> 7\u001b[0m     accuracy \u001b[39m=\u001b[39m qcnn(FashionMNIST, SGD, CrossEntropyLoss())\n\u001b[1;32m      8\u001b[0m     \u001b[39m# print(f\"New ansatz: {accuracy=:.3%}\")\u001b[39;00m\n",
                        "File \u001b[0;32m~/Documents/Spring 2023.nosync/thesis/src/qcnn.py:127\u001b[0m, in \u001b[0;36mQCNN.__call__\u001b[0;34m(self, dataset, optimizer, cost_fn)\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(parameters) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    125\u001b[0m         new_params[\u001b[39m-\u001b[39m\u001b[39mlen\u001b[39m(parameters) :] \u001b[39m=\u001b[39m parameters\n\u001b[0;32m--> 127\u001b[0m parameters \u001b[39m=\u001b[39m train(\n\u001b[1;32m    128\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpredict,\n\u001b[1;32m    129\u001b[0m     optimizer,\n\u001b[1;32m    130\u001b[0m     training_dataloader,\n\u001b[1;32m    131\u001b[0m     cost_fn,\n\u001b[1;32m    132\u001b[0m     initial_parameters\u001b[39m=\u001b[39;49mnew_params,\n\u001b[1;32m    133\u001b[0m )\n\u001b[1;32m    135\u001b[0m accuracy \u001b[39m=\u001b[39m test(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpredict, parameters, testing_dataloader)\n\u001b[1;32m    136\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mnum_layers\u001b[39m=}\u001b[39;00m\u001b[39m, \u001b[39m\u001b[39m{\u001b[39;00maccuracy\u001b[39m=:\u001b[39;00m\u001b[39m.3%\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
                        "File \u001b[0;32m~/Documents/Spring 2023.nosync/thesis/src/training.py:33\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(fn, optimizer, training_dataloader, cost_fn, initial_parameters, total_params)\u001b[0m\n\u001b[1;32m     31\u001b[0m predictions \u001b[39m=\u001b[39m fn(params, data)\n\u001b[1;32m     32\u001b[0m cost \u001b[39m=\u001b[39m cost_fn(predictions, labels)\n\u001b[0;32m---> 33\u001b[0m cost\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     34\u001b[0m opt\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     36\u001b[0m \u001b[39m# if (i == 0) or ((i + 1) % 100 == 0) or (i + 1 == len(training_dataloader)):\u001b[39;00m\n\u001b[1;32m     37\u001b[0m \u001b[39m#     print(f\"{i+1}/{len(training_dataloader)}: {cost=:.03f}\")\u001b[39;00m\n",
                        "File \u001b[0;32m~/Documents/Spring 2023.nosync/thesis/.env/lib/python3.10/site-packages/torch/_tensor.py:488\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    478\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    479\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    480\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    481\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    486\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    487\u001b[0m     )\n\u001b[0;32m--> 488\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    489\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    490\u001b[0m )\n",
                        "File \u001b[0;32m~/Documents/Spring 2023.nosync/thesis/.env/lib/python3.10/site-packages/torch/autograd/__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    192\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    194\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 197\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    198\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    199\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
                        "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
                    ]
                }
            ],
            "source": [
                "%run src/ansatz/v1.py\n",
                "\n",
                "for dim in dims:\n",
                "    qcnn = QCNN(dim, ansatz=Ansatz1())\n",
                "\n",
                "    for _ in range(num_trials):\n",
                "        accuracy = qcnn(FashionMNIST, SGD, CrossEntropyLoss())\n",
                "        # print(f\"New ansatz: {accuracy=:.3%}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "for dim in dims:\n",
                "    qcnn = QCNN_Old(dim)\n",
                "\n",
                "    for _ in range(num_trials):\n",
                "        accuracy = qcnn(FashionMNIST, SGD, CrossEntropyLoss())\n",
                "        print(f\"Old ansatz: {accuracy=:.3%}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# To test:\n",
                "# Dims = 16, 28, 32\n",
                "# Dataset = MNIST, FashionMNIST\n",
                "# All different ansatz\n",
                "# 20 trials each"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.9"
        },
        "orig_nbformat": 4,
        "vscode": {
            "interpreter": {
                "hash": "c7967d85baf232544f00baec60c53642b018367ac4c489b1a4dde60b922cc6fa"
            }
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
