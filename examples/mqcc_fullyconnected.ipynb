{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_features = 32\n",
    "out_features = 16\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Parameter containing:\n",
       " tensor([[ 0.1594, -0.0521,  0.0043, -0.1027, -0.0943, -0.0552,  0.1466, -0.0770,\n",
       "           0.1757,  0.0166,  0.0513, -0.0884,  0.1690, -0.0112, -0.0011,  0.1035,\n",
       "          -0.1184,  0.0792, -0.0940,  0.0448, -0.0268,  0.0602,  0.1435, -0.1468,\n",
       "           0.0125, -0.0509, -0.0590, -0.0944,  0.1577, -0.0261, -0.0684,  0.0261],\n",
       "         [-0.1470,  0.1627,  0.0857,  0.0517, -0.1272,  0.0873, -0.1467, -0.0955,\n",
       "          -0.1554, -0.1357,  0.1632,  0.0747, -0.1634, -0.0167, -0.0083,  0.1468,\n",
       "          -0.1742,  0.0993,  0.0447, -0.0253, -0.0301, -0.0858, -0.1282,  0.0898,\n",
       "          -0.1534, -0.1267,  0.0504,  0.1595, -0.0668,  0.1766,  0.0859, -0.0315],\n",
       "         [-0.0017,  0.1036, -0.0277, -0.1187, -0.1127, -0.0109, -0.0541,  0.1402,\n",
       "          -0.1763, -0.1722, -0.1371, -0.1643,  0.0863,  0.1699,  0.1202, -0.0003,\n",
       "          -0.0847, -0.0298,  0.0474,  0.1285,  0.1625, -0.0697, -0.0654,  0.1098,\n",
       "          -0.0037, -0.0591, -0.0195,  0.0172,  0.0998,  0.1283, -0.0945,  0.0428],\n",
       "         [ 0.0604,  0.0805, -0.1327,  0.1508, -0.1370, -0.0163,  0.0235, -0.0280,\n",
       "          -0.1106,  0.0307, -0.1621, -0.1217, -0.0915, -0.1561,  0.1597, -0.0864,\n",
       "           0.0520, -0.0837,  0.0684, -0.1244, -0.0118,  0.0220,  0.0018,  0.0133,\n",
       "          -0.1029, -0.1277, -0.0418,  0.0479,  0.1698, -0.1172,  0.0930, -0.1634],\n",
       "         [-0.0591, -0.0441,  0.1549, -0.0384,  0.0159, -0.1626, -0.0142,  0.0583,\n",
       "          -0.1448,  0.0826, -0.1517, -0.0087,  0.0587,  0.0868, -0.1370, -0.1154,\n",
       "          -0.1504, -0.0156, -0.1035,  0.1588,  0.0522,  0.1044, -0.0976,  0.0594,\n",
       "          -0.1023,  0.1512,  0.0897,  0.1158, -0.0583, -0.0406,  0.1646,  0.0679],\n",
       "         [ 0.1332,  0.1137,  0.0374,  0.0114, -0.1311,  0.1090, -0.1602, -0.1497,\n",
       "           0.1624, -0.0554, -0.1097, -0.1049,  0.1622, -0.0487, -0.0761,  0.1229,\n",
       "           0.0839, -0.0891,  0.1429, -0.0071, -0.0232,  0.0362, -0.1156, -0.0775,\n",
       "          -0.0136,  0.1182,  0.1038, -0.1520,  0.0087, -0.1554,  0.0044, -0.0176],\n",
       "         [-0.1539, -0.0925, -0.0393,  0.0327, -0.0809,  0.0033,  0.0268,  0.1629,\n",
       "          -0.1460, -0.0920, -0.0142,  0.0626,  0.1636,  0.0597, -0.1004,  0.1366,\n",
       "          -0.1050,  0.1742, -0.0507,  0.0093,  0.0089, -0.0161,  0.1293, -0.0419,\n",
       "           0.1542,  0.0752, -0.0140,  0.0976,  0.1102, -0.1696, -0.0935, -0.0121],\n",
       "         [ 0.0868, -0.1294, -0.1233, -0.1656, -0.0930, -0.1708, -0.1356,  0.1226,\n",
       "          -0.0891, -0.0238,  0.0973,  0.0326,  0.0354, -0.0797,  0.0573, -0.0504,\n",
       "           0.1485,  0.1503, -0.1685, -0.0368, -0.1092,  0.0551, -0.1736, -0.1737,\n",
       "          -0.0115, -0.1484,  0.0921, -0.0195, -0.1664, -0.1691,  0.0430, -0.0069],\n",
       "         [ 0.1465, -0.0663, -0.1203,  0.1301,  0.0499, -0.0958, -0.0969,  0.1092,\n",
       "           0.1736,  0.0650,  0.1440, -0.1464, -0.0112, -0.0164, -0.0148,  0.0842,\n",
       "           0.1524, -0.0051,  0.1191,  0.0282,  0.0984, -0.0797,  0.0829,  0.0717,\n",
       "          -0.1006, -0.1704,  0.0013, -0.1496, -0.0921, -0.1678,  0.0256, -0.1565],\n",
       "         [-0.0706,  0.1058, -0.1247, -0.1327,  0.0976, -0.0048,  0.1627,  0.0677,\n",
       "           0.1615, -0.0507,  0.1252, -0.0481, -0.0846, -0.0576, -0.1671, -0.1506,\n",
       "          -0.1144,  0.0704,  0.1148, -0.1362, -0.0275, -0.0793, -0.0605,  0.1192,\n",
       "          -0.1475,  0.0359, -0.1172, -0.0398,  0.0749,  0.0625,  0.0463, -0.0417],\n",
       "         [-0.1568, -0.1095, -0.0261,  0.0771, -0.1279,  0.1237, -0.0019,  0.1309,\n",
       "          -0.1760,  0.0314, -0.1378,  0.0915,  0.0456, -0.1138, -0.1336, -0.0583,\n",
       "           0.0596, -0.0420, -0.1094,  0.0794, -0.0870,  0.1053,  0.1051, -0.0815,\n",
       "           0.0590, -0.0211, -0.0488,  0.1378, -0.0896, -0.1079,  0.0930, -0.0088],\n",
       "         [ 0.0033,  0.0472, -0.0075,  0.0836, -0.0570,  0.0978,  0.0964, -0.1211,\n",
       "          -0.0950,  0.0282,  0.0974,  0.1632, -0.1584,  0.0203, -0.0817, -0.1052,\n",
       "           0.0851,  0.1158, -0.0076,  0.1533, -0.0613,  0.1501,  0.0788,  0.1539,\n",
       "          -0.1250,  0.1399, -0.1712,  0.0943,  0.1664, -0.0495,  0.0292, -0.0423],\n",
       "         [-0.1372,  0.1305, -0.1667, -0.0319, -0.0566, -0.1222, -0.0271,  0.0031,\n",
       "          -0.0782, -0.0694,  0.0416,  0.1538, -0.0519, -0.1062, -0.0956,  0.0171,\n",
       "           0.1360, -0.0749,  0.1180, -0.1214, -0.1614, -0.0061, -0.1356, -0.1407,\n",
       "          -0.1094,  0.1545, -0.0172,  0.0750, -0.1608,  0.0242, -0.1263,  0.0593],\n",
       "         [ 0.1166, -0.0663,  0.0154,  0.1668, -0.0322,  0.0274, -0.1718, -0.0058,\n",
       "           0.0589,  0.0432, -0.0580,  0.0580,  0.1231,  0.0527,  0.0100, -0.0977,\n",
       "           0.1611,  0.1415, -0.0394, -0.1316, -0.1665,  0.0741, -0.1376,  0.1683,\n",
       "           0.1262, -0.0710, -0.1128,  0.0733, -0.0169,  0.1206,  0.0546, -0.1184],\n",
       "         [-0.1391,  0.0690,  0.0166,  0.0133, -0.0394,  0.1294, -0.0489,  0.0196,\n",
       "          -0.0222,  0.0229, -0.1233, -0.1688, -0.0772, -0.1589, -0.0562,  0.0270,\n",
       "           0.0281,  0.1034, -0.1640, -0.0446,  0.1330, -0.1147,  0.1621, -0.0952,\n",
       "          -0.0670,  0.0284,  0.1724,  0.0149, -0.0798, -0.0711, -0.1070, -0.0495],\n",
       "         [ 0.1592, -0.0821, -0.0678,  0.0349, -0.0828,  0.0095, -0.0780, -0.1488,\n",
       "          -0.1593,  0.0498, -0.0352, -0.1692,  0.1030,  0.0917,  0.1752,  0.0372,\n",
       "          -0.1001,  0.0227,  0.0685, -0.0670,  0.1682, -0.0017, -0.1297, -0.1119,\n",
       "           0.1693, -0.1251,  0.0964,  0.0651,  0.0975,  0.0774,  0.0809, -0.0434]],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([-0.0802, -0.1116,  0.0331,  0.0270,  0.0326,  0.0184,  0.0335, -0.0709,\n",
       "         -0.1405, -0.0329, -0.0138, -0.0402, -0.0229, -0.1512,  0.0736, -0.0907],\n",
       "        requires_grad=True))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.nn import Linear\n",
    "\n",
    "classical = Linear(in_features, out_features)\n",
    "classical.weight, classical.bias\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Parameter containing:\n",
       " tensor([4.6373, 4.3498, 5.5419, 2.6085, 5.0958, 4.5741, 2.8379, 1.5887, 5.1858,\n",
       "         1.8148, 2.5991, 5.2135, 0.2298, 6.1416, 2.0242, 5.9485, 1.8135, 5.6940,\n",
       "         5.2367, 0.3847, 5.4828, 4.3732, 1.7968, 0.3559, 4.7206, 0.1702, 6.0865,\n",
       "         3.3839, 6.1701, 3.3526, 4.7094, 0.4899, 1.3754, 1.1998, 5.3803, 3.2014,\n",
       "         1.0103, 3.4031, 6.2109, 0.9646, 2.1144, 3.7120, 4.1619, 6.1995, 0.9721,\n",
       "         1.3407, 0.6436, 2.1752, 2.6144, 5.5595, 2.2695, 2.4033, 0.4929, 0.5402,\n",
       "         4.4009, 1.8097, 4.0728, 1.3805, 6.1856, 5.1745, 4.8691, 2.4932, 4.7154,\n",
       "         2.4068, 2.6886, 0.3087, 5.2120, 4.1594, 1.4122, 2.3909, 1.1883, 0.2230,\n",
       "         1.0002, 3.9882, 5.8532, 3.2120, 0.0601, 2.7478, 0.1837, 0.5664, 5.3580,\n",
       "         5.3317, 3.8163, 3.9495, 2.1877, 5.1333, 1.5083, 2.9834, 5.4565, 1.3985,\n",
       "         5.0403, 4.0492, 2.0382, 3.3500, 3.2097, 1.2236, 3.6530, 5.0666, 2.1805,\n",
       "         4.9286, 0.5437, 5.3566, 0.4286, 6.2682, 3.4079, 3.4614, 5.1726, 1.7141,\n",
       "         0.6255, 2.3292, 1.0692, 4.6321, 2.9442, 3.0274, 5.9671, 5.0032, 0.1407,\n",
       "         5.8896, 0.1573, 3.2353, 1.7616, 2.8938, 1.4553, 4.5619, 4.4867, 3.4439,\n",
       "         5.2219, 4.4590, 1.5060, 2.3761, 3.4843, 2.0776, 5.6639, 5.8638, 5.6800,\n",
       "         5.9194, 1.5474, 5.0634, 2.4240, 5.1769, 0.9373, 1.0913, 5.5152, 0.7599,\n",
       "         2.6609, 0.4678, 3.2926, 3.0665, 0.5759, 4.0070, 1.8096, 5.5024, 1.5274,\n",
       "         5.1812, 3.5880, 5.3888, 3.7317, 2.7783, 2.1629, 3.3876, 3.2759, 2.4009,\n",
       "         1.8341, 1.5590, 2.9555, 3.3764, 1.0709, 1.5882, 3.0382, 4.9887, 2.7037,\n",
       "         4.9986, 0.9975, 5.1429, 0.1083, 1.7033, 5.4177, 2.3633, 3.4194, 2.7115,\n",
       "         0.9083, 4.8936, 3.8726, 0.7243, 3.4166, 1.7508, 1.0459, 0.1692, 2.6993,\n",
       "         1.8747, 1.4086, 5.3410, 2.9578, 0.6910, 2.8756, 0.3314, 2.5755, 3.5973,\n",
       "         1.2675, 1.7290, 5.0318, 1.4020, 2.3876, 6.2279, 0.3567, 2.0738, 5.1859,\n",
       "         5.4134, 1.7512, 5.6209, 4.4586, 5.4350, 0.7172, 1.0017, 0.4974, 0.2199,\n",
       "         4.0933, 2.4211, 2.1441, 5.3125, 0.6936, 2.9957, 0.9786, 0.7472, 3.7065,\n",
       "         3.7010, 4.3866, 1.9971, 5.4222, 5.2714, 5.5914, 2.1069, 1.2276, 3.1637,\n",
       "         4.9655, 0.1923, 5.7939, 3.3682, 1.9331, 3.8023, 6.2277, 1.9777, 3.2410,\n",
       "         0.7779, 0.8706, 1.9572, 0.1345, 3.4618, 3.8542, 5.1731, 0.0664, 5.6975,\n",
       "         0.1389, 2.5018, 0.7482, 3.3621, 3.7026, 2.9657, 2.4075, 0.6120, 3.0235,\n",
       "         1.6383, 4.2858, 4.0042, 3.8338, 3.3733, 2.0311, 0.2585, 2.2080, 3.2803,\n",
       "         0.3320, 5.4400, 0.0607, 1.9950, 4.7740, 2.7776, 2.6692, 1.4214, 0.4237,\n",
       "         6.2045, 6.2326, 3.9415, 4.9972, 6.0066, 2.6753, 1.0196, 4.4211, 5.3485,\n",
       "         5.1440, 4.1145, 5.8128, 5.3012, 4.1883, 3.4175, 5.3420, 1.1225, 3.8784,\n",
       "         1.9751, 5.6610, 3.8055, 0.6743, 2.5362, 0.2311, 0.2016, 2.5943, 6.1757,\n",
       "         5.1286, 5.3315, 1.1544, 3.8419, 0.8528, 3.3877, 5.8653, 3.4218, 5.5464,\n",
       "         4.3438, 3.7190, 6.2624, 4.3564, 4.8124, 6.1299, 0.9699, 2.2546, 1.6714,\n",
       "         5.8328, 4.1525, 2.6983, 0.6588, 3.9042, 1.0770, 5.9162, 0.3111, 3.7341,\n",
       "         2.1990, 1.2666, 4.4369, 2.0189, 1.0690, 6.1648, 1.2804, 3.7451, 1.8884,\n",
       "         3.0764, 2.5290, 3.9613, 1.1283, 4.9810, 0.5566, 0.2490, 5.6750, 5.7630,\n",
       "         4.3416, 4.6003, 2.4866, 3.4771, 2.7261, 0.7192, 4.9803, 1.6063, 3.9213,\n",
       "         3.2428, 5.8670, 1.5014, 0.7489, 1.6481, 0.4007, 0.8091, 3.4111, 5.6218,\n",
       "         0.9574, 4.9704, 1.6938, 1.9115, 3.6149, 0.6578, 0.5436, 5.8368, 4.3832,\n",
       "         6.1659, 3.9618, 3.5659, 5.0860, 1.5695, 4.4811, 1.6147, 3.6353, 3.7499,\n",
       "         2.4547, 0.6142, 1.2601, 3.2276, 1.7537, 0.6199, 0.8575, 5.8472, 0.0883,\n",
       "         1.5194, 0.9624, 0.0791, 5.2892, 5.9209, 4.8579, 4.7647, 0.5876, 0.0823,\n",
       "         0.3596, 0.3737, 2.9072, 0.5725, 1.4022, 1.1794, 2.9037, 1.9783, 6.1780,\n",
       "         1.2627, 4.1887, 0.4694, 5.1242, 0.2406, 6.2433, 3.5530, 2.6248, 2.7498,\n",
       "         4.1232, 3.8633, 1.7017, 1.7956, 0.1733, 4.8350, 2.5840, 6.1381, 0.4214,\n",
       "         5.5001, 2.2888, 3.1598, 1.2549, 4.5111, 0.8979, 5.9807, 5.4044, 0.0422,\n",
       "         2.3792, 6.1760, 4.8021, 5.4844, 3.9808, 4.5686, 6.1996, 3.8357, 4.5534,\n",
       "         6.0729, 1.3526, 2.4093, 2.9178, 2.0255, 3.4012, 2.0152, 0.4840, 2.2703,\n",
       "         1.9112, 4.0590, 1.4232, 6.1085, 3.6964, 0.1399, 3.1287, 1.3712, 3.1981,\n",
       "         5.9350, 2.4873, 2.5669, 1.9350, 0.3157, 5.6585, 2.9371, 6.2085, 4.1361,\n",
       "         2.2541, 5.8352, 3.3427, 5.8753, 1.4908, 0.5445, 0.1826, 1.3750, 5.1937,\n",
       "         2.7892, 5.3419, 4.3160, 3.1814, 3.4246, 2.1334, 0.7967, 3.1341, 3.0898,\n",
       "         4.1810, 4.0023, 4.3913, 1.0377, 2.1037, 4.0196, 0.0392, 3.8587, 6.1778,\n",
       "         6.0582, 1.1619, 3.0165, 2.3311, 3.4635, 2.3472, 3.5435, 5.6844],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([ 0.0962,  0.1349,  0.1208,  0.0805,  0.1541, -0.1733,  0.1618,  0.0790,\n",
       "         -0.0159, -0.1424,  0.0738,  0.1714, -0.1647,  0.1211, -0.0442,  0.0326],\n",
       "        requires_grad=True))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from qcc.ml.quantum import FullyConnectedLayer, ConvolutionFilter\n",
    "\n",
    "quantum = FullyConnectedLayer(in_features, out_features, U_kernel=ConvolutionFilter)\n",
    "quantum.fc.weight, quantum.bias\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "weight = torch.abs(classical.weight.T / torch.norm(classical.weight, dim=1)).T\n",
    "bias = classical.bias\n",
    "\n",
    "weight = torch.nn.Parameter(weight)\n",
    "bias = torch.nn.Parameter(bias)\n",
    "\n",
    "classical.weight, classical.bias = weight, bias\n",
    "quantum.fc.weight, quantum.bias = torch.nn.Parameter(weight.flatten()), bias\n",
    "quantum.norm = torch.nn.Parameter(torch.ones_like(bias))\n",
    "\n",
    "assert((classical.weight.flatten() == quantum.fc.weight).all())\n",
    "assert((classical.bias == quantum.bias).all())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = torch.nn.init.uniform_(torch.zeros(in_features))\n",
    "data = data / torch.abs(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4.6539, 4.9559, 4.8065, 4.8797, 5.0050, 4.8798, 4.7300, 4.8756, 4.7849,\n",
       "        5.0613, 5.0462, 4.8979, 4.9099, 4.7303, 4.8195, 4.8902],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(classical.weight @ data) + classical.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([4.6539, 4.9559, 4.8065, 4.8797, 5.0050, 4.8798, 4.7300, 4.8756, 4.7849,\n",
       "         5.0613, 5.0462, 4.8979, 4.9099, 4.7303, 4.8195, 4.8902],\n",
       "        grad_fn=<ViewBackward0>),\n",
       " tensor([[4.6539, 4.9559, 4.8065, 4.8797, 5.0050, 4.8798, 4.7300, 4.8756, 4.7849,\n",
       "          5.0613, 5.0462, 4.8979, 4.9099, 4.7303, 4.8195, 4.8902]],\n",
       "        grad_fn=<ToCopyBackward0>))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classical(data), quantum(data.unsqueeze(0))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
